<html xmlns="http://www.w3.org/1999/xhtml"> <head> <style type="text/css">
      	     		 
      	    a                          {background : #ffffff; }
      	    article                    {border-style : dotted; border-width : 2px; }
		 	
		 	div                        {background : #ffffcc;}
		 	div.abstract-title         {font-weight : bold ; font-size : 16pt;}
		 	div.ack                    {border-style : solid ; border-color : red; margin : 2em; }
		 	div.article-meta           {border-style : solid; border-width : 1 pt; margin 1em;}
		 	div.boxed-text             {margin : 5em; border-style : solid;}
		 	div.contrib-group          {margin : 1em; }
		 	div.fig                    {border-style : solid; border-width : 2px; margin : 2em; }
		 	div.funding                {font-weight : bold ; font-size : 16pt;}
		 	div.given-names            {font-style : italic;}
		 	div.intro                  {border-style : inset; margin : 5px;}
		 	div.introduction           {border-style : inset; margin : 5px;}
		 	div.journal-meta           {border-style : solid; border-width : 1 pt; margin 1em;}
		 	div.journal-title-group    {background : #ffeeee;}
		 	div.article-title          {font-weight : bold ; font-size : 18pt;}
		 	div.kwd                    {font-style : italic}
		 	div.meta-name              {font-weight : bold ; font-size : 16pt;}
		 	div.name                   {font-weight : bold;}
		 	div.alt-title              {font-style : italic; font-size : 12px;}
		 	
		 	div.sec                    {border : 2px; margin 5px; padding 2px;}
		 	div.title                  {font-family : courier; font-weight : bold;
		 	                            font-size : 16pt; margin : 5px;}
		 	
		 	div.materials_methods      {border-style : double; margin : 5px; }
		 	div.methods                {border-style : double; margin : 5px; }
		 	
		 	div.results                {border-style : solid; margin : 5px;}
		 	div.background             {border-style : dotted; margin : 5px;}
		 	
		 	div.discussion             {border-style : groove; margin : 5px;}
		 	div.conclusion             {border-style : ridge; margin : 5px;}
		 	div.conclusions             {border-style : ridge; margin : 5px;}
		 	div.supplementary-material {border-style : inset; margin : 5px;}
		 	div.abbreviations          {border-style : double; border-color : red; margin : 5px;}
		 	div.competinginterests     {border-style : double; border-color : blue; margin : 5px;}
		 	div.acknowledgements       {border-style : double; border-color : green; margin : 5px;}
		 	div.authors_contributions   {border-style : double; border-color : purple; margin : 5px;}
		 	
		 	div.publisher              {border-style : outset; margin : 5px;}
		 	div.fn-type-conflict       {background : #f88; }
		 	div.fn-type-con            {background : #ddf; }
		 	div.fn-type-other          {background : #ddd; }
		 	
		 	div.unknown                {background : #ffd;
									 	  border-style : solid;
									 	  border-width : 1px;
									 	  padding : 2 px;}
		 	  
      	    table                      {background : #ffffdd;}
		 	tr                         {background : #ddddff; padding : 1px;}
		 	
		 	span                       {background : #ffcccc;}
		 	
		 	span.citation-author       {font-family : helvetica; background : #ffeeee;}
		 	span.collab                {background : #ddffff; }
		 	span.comment               {font-family : courier; font-size : 6px; background : #ffaaff;}
		 	span.contrib               {background : #ffffff;}
		 	span.corresp               {background : #ddffdd; }
		 	span.doi                   {background : #ffffff;}
		 	span.email                 {font-family : courier; }
		 	span.etal                  {font-style : italic;}
		 	span.fpage                 {font-family : courier;}
		 	span.given-names           {background : #ffffff;}
		 	span.iso-abbrev            {background : #ffffff;}
		 	span.issn-epub             {background : #ffffff;}
		 	span.issn-ppub             {background : #ffffff;}
		 	span.journal-title         {background : #ffffff;}
		 	span.lpage                 {font-family : courier;}
		 	span.mixed-article-title   {font-style : italic ;}
		 	span.nlm-ta                {background : #ffffff;}
		 	span.pmc                   {background : #ffffff;}
		 	span.pmcid                 {background : #ffffff;}
		 	span.pmid                  {background : #ffffff;}
		 	span.publisher             {background : #ffffff;}
		 	span.publisher-id          {background : #ffffff;}
		 	span.publisher-name        {background : #ffffff;}
		 	span.source                {background : #ffffff;}
		 	span.subject               {background : #ffffff;}
		 	span.surname               {background : #ffffff;}
		 	span.volume                {font-family : courier; font-weight : bold;}
		 	span.year                  {font-family : courier ; font-style : italic;}
			</style> </head> <body> <div class="front" title="front"> <div class="journal-meta" tagx="journal-meta" title="journal-meta"><span class="nlm-ta" title="nlm-ta">NPJ Sci Learn</span><span class="iso-abbrev" title="iso-abbrev">NPJ Sci Learn</span><div class="journal-title-group" tagx="journal-title-group" title="journal-title-group"><span class="journal-title" tagx="journal-title" title="journal-title">NPJ Science of Learning</span></div><span class="issn-epub" tagx="issn" title="issn-epub">2056-7936</span><div class="publisher" tagx="publisher" title="publisher"><span class="publisher-name" tagx="publisher-name" title="publisher-name">Nature Publishing Group UK</span><span class="publisher-loc" tagx="publisher-loc" title="publisher-loc">London</span></div> </div> <div class="article-meta" tagx="article-meta" title="article-meta"><span class="pmcid" title="pmcid"> pmcid: <a href="http://www.ncbi.nlm.nih.gov/pubmed/7235041">7235041</a></span><span class="publisher-id" title="publisher-id">65</span><span class="doi" title="doi"> doi: <a href="https://dx.doi.org/10.1038/s41539-020-0065-x">10.1038/s41539-020-0065-x</a></span><div class="article-categories" title="article-categories">
: <span class="subject" title="subject">Article</span></div> <div class="title-group"> <div class="article-title" title="article-title">
Using the force: STEM knowledge and experience construct shared neural representations of engineering concepts</div> </div> <div class="contrib-group" title="contrib-group"><span class="contrib" title="contrib"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Cetron</span><span class="given-names" tagx="given-names" title="given-names">Joshua S.</span></span><a href="#Aff1">1</a><a href="#Aff2">2</a></span><span class="contrib" title="contrib"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Connolly</span><span class="given-names" tagx="given-names" title="given-names">Andrew C.</span></span><a href="#Aff3">3</a></span><span class="contrib" title="contrib"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Diamond</span><span class="given-names" tagx="given-names" title="given-names">Solomon G.</span></span><a href="#Aff4">4</a></span><span class="contrib" title="contrib"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">May</span><span class="given-names" tagx="given-names" title="given-names">Vicki V.</span></span><a href="#Aff4">4</a></span><span class="contrib" title="contrib"><span class="contrib-id" tagx="contrib-id" title="contrib-id">http://orcid.org/0000-0002-6558-3118</span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Haxby</span><span class="given-names" tagx="given-names" title="given-names">James V.</span></span><a href="#Aff3">3</a></span><span class="contrib" title="contrib"><span class="contrib-id" tagx="contrib-id" title="contrib-id">http://orcid.org/0000-0001-9829-2029</span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kraemer</span><span class="given-names" tagx="given-names" title="given-names">David J. M.</span></span><span class="address" title="address"><span class="email" tagx="email" title="email">David.J.M.Kraemer@Dartmouth.edu</span></span><a href="#Aff2">2</a></span><span class="citation_author_institution" id="Aff1">[1], <div class="institution-wrap" title="institution-wrap"><span class="institution-id" title="institution-id">000000041936754X</span><span class="institution-id" title="institution-id">grid.38142.3c</span><span class="institution" title="institution">Department of Psychology, </span><span class="institution" title="institution">Harvard University, </span></div></span><span class="citation_author_institution" id="Aff2">[2], <div class="institution-wrap" title="institution-wrap"><span class="institution-id" title="institution-id">0000 0001 2179 2404</span><span class="institution-id" title="institution-id">grid.254880.3</span><span class="institution" title="institution">Department of Education, </span><span class="institution" title="institution">Dartmouth College, </span></div></span><span class="citation_author_institution" id="Aff3">[3], <div class="institution-wrap" title="institution-wrap"><span class="institution-id" title="institution-id">0000 0001 2179 2404</span><span class="institution-id" title="institution-id">grid.254880.3</span><span class="institution" title="institution">Department of Psychological and Brain Sciences, </span><span class="institution" title="institution">Dartmouth College, </span></div></span><span class="citation_author_institution" id="Aff4">[4], <div class="institution-wrap" title="institution-wrap"><span class="institution-id" title="institution-id">0000 0001 2179 2404</span><span class="institution-id" title="institution-id">grid.254880.3</span><span class="institution" title="institution">Thayer School of Engineering, </span><span class="institution" title="institution">Dartmouth College, </span></div></span></div><span class="pub-date-epub" title="pub-date-epub">epub: <span>2020-5-5</span></span><span class="pub-date-pmc-release" title="pub-date-pmc-release">pmc-release: <span>2020-5-5</span></span><span class="pub-date-collection" title="pub-date-collection">collection: <span>2020</span></span><span class="volume" tagx="volume" title="volume">5</span><span class="elocation-id" tagx="elocation-id" title="elocation-id">6</span><span class="history" title="history"><span class="received" title="received">received: 2019-8-17</span><span class="accepted" title="accepted">accepted: 2020-3-24</span></span><div class="permissions"><span class="copyright" title="copyright">(C) , </span><span class="license" title="license"><span class="license-p" title="license-p"><b>Open Access</b> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <a href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</a>.</span></span></div> <div class="abstract" title="abstract"> <div class="abstract-title" title="abstract-title">
Abstract</div> <p id="Par1">How does STEM knowledge learned in school change students’ brains? Using fMRI, we presented photographs of real-world structures to engineering students with classroom-based knowledge and hands-on lab experience, examining how their brain activity differentiated them from their "novice" peers not pursuing engineering degrees. A data-driven MVPA and machine-learning approach revealed that neural response patterns of engineering students were convergent with each other and distinct from novices’ when considering physical forces acting on the structures. Furthermore, informational network analysis demonstrated that the distinct neural response patterns of engineering students reflected relevant concept knowledge: learned categories of mechanical structures. Information about mechanical categories was predominantly represented in bilateral anterior ventral occipitotemporal regions. Importantly, mechanical categories were not explicitly referenced in the experiment, nor does visual similarity between stimuli account for mechanical category distinctions. The results demonstrate how learning abstract STEM concepts in the classroom influences neural representations of objects in the world.</p> </div> <div class="kwd-group"> <div class="title" tagx="title" title="title">
Subject terms</div> <div class="kwd" title="kwd">
Human behaviour</div> <div class="kwd" title="kwd">
Learning and memory</div> </div> <div class="custom-meta-group" title="custom-meta-group"> <meta name="issue-copyright-statement" value="© The Author(s) 2020" /> </div> </div> </div> <div class="body" title="body"> <div class="introduction" title="introduction"> <div class="title" tagx="title" title="title">
Introduction</div> <p id="Par2">Learning changes perception. Over the course of learning, advanced students and experts grow to see the world differently from novice learners as a function of their knowledge and prior experience (e.g., chess<sup><a href="#CR1">1</a></sup>, physics<sup><a href="#CR2">2</a></sup>). For example, seminal work in cognitive psychology has demonstrated that when given the same set of physics practice problems, individuals with advanced knowledge of physics categorize the problems according to the abstract concept knowledge underlying each problem, whereas novices categorize problems according to the perceivable surface-level features of each problem<sup><a href="#CR2">2</a></sup>. Although a large body of neuroscience research has focused on the neural representation of visually perceivable categories<sup><a href="#CR3">3</a>–<a href="#CR6">6</a></sup>, the way in which abstract STEM concept knowledge is represented in the brain remains less explored. Here, using a cross-sectional design, we investigate how prior classroom-based and lab-based learning experiences influence the neural patterns that represent abstract conceptual categories when presented with naturalistic stimuli.</p> <p id="Par3">To date, only a few neuroimaging studies have investigated physics and engineering concept knowledge<sup><a href="#CR7">7</a>–<a href="#CR10">10</a></sup>. Results of this research implicate dorsal stream regions—including motor cortex—in the explicit retrieval of task-specific physics knowledge. However, in a naturalistic test of conceptual understanding, an important outcome of successful learning should be the ability to observe real-world stimuli and implicitly retrieve conceptual category information not available to novices<sup><a href="#CR1">1</a>,<a href="#CR2">2</a></sup>. Because prior fMRI studies of physics knowledge have focused on explicit retrieval of information about non-naturalistic stimuli, the question of how abstract physics and engineering knowledge is implicitly activated by real-world stimuli remains unresolved.</p> <p id="Par4">Within other concept domains, numerous fMRI studies have tied both explicit and implicit visual categorization—including categories based on real-world stimuli—to patterns of brain activity in the ventral visual stream<sup><a href="#CR3">3</a>–<a href="#CR6">6</a>,<a href="#CR11">11</a></sup>. For example, Connolly et al.<sup><a href="#CR3">3</a></sup> have demonstrated that patterns of neural activity in ventral occipitotemporal cortex can reliably classify images of animals (viewed during an incidental task) into abstract conceptual categories. In the present study, we apply this analytical approach to the domain of mechanical engineering. Our aim is to identify patterns of brain activity associated with real-world stimuli that implicitly reflect learned abstract mechanical category information.</p> <p id="Par5">In pursuit of this aim, we recruited two groups of participants: a group of undergraduate mechanical engineering students with knowledge drawn from engineering lecture courses and hands-on laboratory experiences, and a control group of their peers: novice students at the same university matched for educational attainment. (Note that engineering students at the present institution do not have separate admissions criteria from students in other disciplines; the distinction between novice and engineering students in the present study is simply that engineering students had completed certain engineering and physics courses, whereas novice students had not elected any advanced courses in engineering or physics.) Both groups performed an fMRI concept knowledge task in which participants evaluated the Newtonian forces acting on a set of real-world structures. Using multivariate pattern analysis (MVPA) of neuroimaging data, we first identified convergent patterns of neural activity among engineering students and among novices. Then, for each group, we performed an informational network analysis<sup><a href="#CR7">7</a></sup>, a variant of representational similarity analysis (RSA<sup><a href="#CR12">12</a></sup>), to query those activity patterns for the presence of concept knowledge information about the mechanical categories of structures. As a control, we also evaluated the contribution of bottom-up perceptual information to these neural patterns using a forward-encoding model of primary visual cortex (HMAX<sup><a href="#CR13">13</a></sup>). In this way, we identified reliable neural patterns reflective of mechanical category knowledge elicited by real-world stimuli. By comparing these patterns across the two groups of students, our results reveal the influence of prior learning experiences on the neural representation of abstract STEM concepts.</p> </div> <div class="results" title="results"> <div class="title" tagx="title" title="title">
Results</div> <div class="engineeringstudentsdemonstrateknowledgeofnewtonianforces" title="sec"> <div class="title" tagx="title" title="title">
Engineering students demonstrate knowledge of Newtonian forces</div> <p id="Par6">Figure <a href="#Fig1">1</a> shows the trial structure and behavioral results from the free body diagram (FBD) task that participants completed during fMRI scan. The FBD task required participants to evaluate the Newtonian forces interacting with real-world structures (see "Methods" section for a complete description of the FBD task). On average, engineering students significantly outperformed novices on this task (<i>M</i><sub>eng</sub> = 76%, <i>M</i><sub>nov</sub> = 66%, <i>t</i>(29) = 2.44, <i>p</i> = 0.02; Fig. <a href="#Fig1">1b</a> inset). A linear mixed-effects model revealed significant main effects of both run (<i>β</i> = 0.03, SE = 0.01, <i>p</i> = 0.01) and group (engineering students &amp;gt; novices, <i>β</i> = 0.24, SE = 0.06, <i>p</i> = 0.0005). There was also a significant two-way interaction (<i>β</i> = 0.05, SE = 0.01, <i>p</i> = 0.0005) in which novices improved more than engineering students over time (∆<sub>eng</sub> = 7.55%, ∆<sub>nov</sub> = 23.61%, <i>t</i>(29) = 3.47, <i>p</i> = 0.002). On the individual runs, the greatest difference in performance between groups was at run 1 (<i>M</i><sub>eng</sub> = 74%, <i>M</i><sub>nov</sub> = 53%, <i>t</i>(29) = 4.18, <i>p</i> = 0.0002). By run 4, both groups demonstrated mastery of the FBD task, and did not perform significantly differently from each other (<i>t</i>(29) = 1.34, <i>p</i> = 0.19). </p><div class="fig" title="fig"> <div class="figure" title="figure"><a href="http://doi.org/"><span class="label" tagx="label" title="label">Fig. 1</span></a></div>   <div class="title" tagx="title" title="title">
Engineering students significantly outperformed novices on the FBD task at the first fMRI run as well as on average.</div> <p><b>a</b> FBD task structure for the fMRI paradigm. <b>b</b> FBD task results by group. By the final fMRI run, novices improved enough on the FBD task that they no longer significantly differed from engineering students in performance. Error bars indicate standard error of the mean. Note: The photos used in this figure are credited to Vermont Timber Works Inc. (the building awning stimulus example) and Steve Morgan (the truss bridge example stimulus). These images were edited and reused here with permission under the Creative Commons Attribution-ShareAlike 3.0 Unported license (<a href="https://creativecommons.org/licenses/by-sa/3.0/legalcode">https://creativecommons.org/licenses/by-sa/3.0/legalcode</a>). Edits to these images were made by the authors, including the red highlighting and force arrow labels that were applied for use in the free body diagram task. All other elements of Fig. 1 were created by the authors.</p>   </div> <p /> </div> <div class="overviewofneuralresults" title="sec"> <div class="title" tagx="title" title="title">
Overview of neural results</div> <p id="Par7">The neural results discussed here will consider only fMRI results from the first fMRI run. Task performance at fMRI run 1 maximally reflects the variance in prior knowledge and experience among participants. Consequently, fMRI data from run 1 are the most likely to show distinct patterns of brain activity associated with concept knowledge for engineering students compared to novices, if such a distinction should occur at all. In contrast, by fMRI run 4 we observe that participants from both groups have mastered the FBD task. It is therefore difficult to dissociate the effects of task-specific learning from the effects of prior knowledge and experience in any analysis of fMRI data from run 4. Thus, neural data from run 1 are analyzed here to identify the distinct and overlapping patterns of neural activity by group (Fig. <a href="#Fig2">2</a>) and the neural patterns reflecting mechanical and visual information by group (Fig. <a href="#Fig3">3</a>). </p><div class="fig" title="fig"> <div class="figure" title="figure"><a href="http://doi.org/"><span class="label" tagx="label" title="label">Fig. 2</span></a></div>   <div class="title" tagx="title" title="title">
Novices and engineering students show distinct and overlapping patterns of neural activity.</div> <p> The schematic to the left describes the analytical procedure: First, in each subject’s brain, we computed a data-driven neural dissimilarity matrix (DM) at each surface node, consisting of the pairwise comparisons between stimulus items. Then, these individual subjects’ node-level DMs were correlated across subjects within each group. A noise threshold was applied using the negative extent of the intersubject correlations as a chance estimate. Finally, results were subjected to a spatial cluster correction on the cortical surface, retaining only clusters of at least five contiguous surface nodes. The brain surface maps on the right show regions where intersubject DM correlations were above the chance threshold for engineering students only (green), novices only (blue), and both groups (red). Anatomical reference labels have been added for M1, vOT, intraparietal sulcus (IPS), and calcarine sulcus (V1). Engineering students showed unique representational convergence in motor regions including M1, as well as ventral PFC and inferior parietal regions. Novice-specific regions included more anterior frontal regions and more dorsal parietal regions. Groups overlapped in representational convergence in occipital regions, including vOT until the most anterior regions.</p>   </div> <div class="fig" title="fig"> <div class="figure" title="figure"><a href="http://doi.org/"><span class="label" tagx="label" title="label">Fig. 3</span></a></div>   <div class="title" tagx="title" title="title">
Neural patterns reflect mechanical category information within group-specific regions, whereas visual similarity information is represented in overlapping regions.</div> <p> Mechanical category informational gradients for engineering students (top left) and novices (top right), accompanied by the model DM for the mechanical categories of the stimuli. Engineering students showed a gradient of increasing mechanical category information moving from posterior-to-anterior in vOT, peaking in a bilateral anterior vOT informational network. Engineering students also showed a gradient of increasing mechanical category information in the dorsal stream moving toward M1. Novices showed no significant mechanical category information, but still showed a similar posterior-to-anterior gradient of increasing mechanical category information in the ventral stream. In the bottom panel, complementary results from the visual similarity analysis are shown, alongside the visual similarity model DM produced by HMAX. Both groups show peak visual similarity information in V1, with visual similarity information increasing along an anterior-to-posterior gradient in the ventral stream (the opposite direction from the mechanical category information gradient).</p>   </div> <p /> </div> <div class="novicesandengineeringstudentsshowdistinctandoverlappingpatternsofneuralactivity" title="sec"> <div class="title" tagx="title" title="title">
Novices and engineering students show distinct and overlapping patterns of neural activity</div> <p id="Par8">In order to identify whether engineering students and novices showed unique patterns of brain activity when considering the stimulus items during the FBD task, we used a data-driven multivariate intersubject correlation analysis to discover any brain regions where participants within a group showed convergent neural representations of the stimulus items. For each individual participant at every surface node in the brain, we computed a dissimilarity matrix (DM) from the pairwise comparisons of item-level beta estimates at each node. Then, separately for each group, we computed the average (<i>z</i>-transformed) intersubject correlations of the DMs at each node. Finally, we used the negative extent of the correlation results as an estimate of the range of correlation values attributable to chance, resulting in a noise threshold of <i>z</i> = 0.02 (see <a href="#MOESM1">Supplementary Methods</a> and Supplementary Fig. <a href="#MOESM1">1</a> for a validation of the noise threshold using permuted null distributions of intersubject correlation values). For each group, nodes with an average intersubject DM correlation of <i>z</i> &amp;gt; 0.02 (cluster-corrected) are shown in Fig. <a href="#Fig2">2</a>, alongside a schematic of the analytical pipeline described here (whole-brain maps of the average intersubject correlation values before thresholding are shown in Supplementary Fig. <a href="#MOESM1">2</a>). Importantly, this analysis was entirely data-driven; at no point was an a priori representational model introduced. (This analysis is described in full detail in the "Methods" section; see the subsection "Multivariate neural pattern analysis", steps 1 and 2).</p> <p id="Par9">Out of 20,484 total nodes across the brain surface, there were 1,863 nodes where only engineering students showed above-threshold intersubject correlations (Fig. <a href="#Fig2">2</a>, green), 842 nodes where only novices showed above-threshold intersubject correlations (Fig. <a href="#Fig2">2</a>, blue), and 2,426 nodes where both groups show above-threshold intersubject correlations (Fig. <a href="#Fig2">2</a>, red). Representational convergence among engineering students was notably localized to motor regions including primary motor cortex (M1), portions of ventral prefrontal cortex (PFC), and inferior parietal regions. Representational convergence among novices was predominantly localized to more anterior frontal regions and superior parietal regions. The representational convergence maps for the two groups overlapped in regions including visual cortex, dorsal occipital cortex, and ventral occipitotemporal (vOT) cortex, although the groups remained distinct in the most anterior representations in vOT.</p> </div> <div class="neuralpatternsreflectmechanicalcategoryknowledgeinengineeringstudents" title="sec"> <div class="title" tagx="title" title="title">
Neural patterns reflect mechanical category knowledge in engineering students</div> <p id="Par10">To unpack the neural distributions of mechanical and visual information for each group within the regions of distinct and overlapping neural representations identified in Fig. <a href="#Fig2">2</a>, we computed an informational network analysis (see subsection "Multivariate neural pattern analysis", steps 3 and 4 in the "Methods" section). First, for each group, we identified networks of brain regions within each of the distinct and overlapping areas with similar representations for the stimulus items, independent of any a priori representational model. Within the regions of high representational convergence among engineering students, a split-half repeated cross-validation procedure yielded 65 informational networks for engineering students and 67 informational networks for novices. Within the regions of high representational convergence for novices, the same procedure yielded 46 informational networks for engineering students and 28 informational networks for novices. Finally, within the regions where both groups showed high representational convergence, we identified 100 informational networks for each group.</p> <p id="Par11">After identifying the informational networks at the group level, we returned to the level of the individual participant. We computed the average representational DM for each informational network separately for each participant. The informational network DMs for each participant were then correlated with a mechanical category information model DM, as well as a visual similarity model DM as a control analysis. The mechanical category DM was generated using the pairwise similarity ratings of a mechanical engineering expert, and delineated three discrete categories of mechanical structures into which the stimulus set could be divided: cantilevers, trusses, and vertical loads. The visual similarity model DM for the stimulus items was generated using the HMAX forward-encoding model of primary visual cortex. Finally, we computed a summary statistic for the mechanical and visual correlation results at the group level using one-sample <i>t</i> tests against zero.</p> <p id="Par12">Figure <a href="#Fig3">3</a> shows the cortical surface maps for mechanical category representations and the visual similarity representations for each group. Engineering students as a group showed a significant peak correlation with the mechanical category representation model in a bilateral anterior vOT informational network (<i>t</i>(15) = 2.27, <i>p</i> = 0.038), with a nonsignificant secondary peak in an M1 informational network (<i>t</i>(15) = 1.93, <i>p</i> = 0.073). Novices as a group showed a nonsignificant mechanical category representation peak in a right anterior vOT informational network (<i>t</i>(14) = 2.06, <i>p</i> = 0.058). Peak mechanical category representations for both groups came from group-specific distinct regions: the peak bilateral vOT and left M1 informational networks for engineering students came from areas with high representational convergence only among engineering students, suggesting that convergence among engineering students’ representations in those brain regions reflects a convergence upon mechanical representations. Figure <a href="#Fig4">4</a> shows the peak informational networks for mechanical category information and visual information along the informational gradients shown in Fig. <a href="#Fig3">3</a>, identifying the distinct and overlapping regions from Fig. <a href="#Fig2">2</a> to which those peak regions belonged. The peak right vOT informational network for novices came from an area with high representational convergence only among novices. Additionally, engineering students showed a gradient of increasing mechanical category information moving from posterior-to-anterior in the ventral stream in the direction of the vOT mechanical representational peak, as well as a similar gradient in the dorsal stream in the direction of M1. Novices showed a similar posterior-to-anterior gradient of increasing mechanical category representation along the ventral stream toward the nonsignificant mechanical representational peak in right vOT. </p><div class="fig" title="fig"> <div class="figure" title="figure"><a href="http://doi.org/"><span class="label" tagx="label" title="label">Fig. 4</span></a></div>   <div class="title" tagx="title" title="title">
The peak regions for each mechanical category informational gradient shown in Fig. <a href="#Fig3">3</a> each came from informational networks localized to group-specific regions (as identified in Fig. <a href="#Fig2">2</a>).</div> <p> Engineering students showed mechanical informational gradient peaks in a bilateral vOT informational network and an M1 informational network, each of which were localized to regions where engineers showed uniquely high representational convergence. Novices showed a mechanical informational gradient peak in a right vOT informational network, which was localized to a region where novices showed uniquely high representational convergence.</p>   </div> <p /> <p id="Par13">In an almost exact reversal from the results of the mechanical category analysis, the gradient of visual representations along the ventral stream for both groups showed visual similarity information increased from anterior-to-posterior along the ventral stream. Significant peak informational networks from the visual similarity analysis in both engineering students (<i>t</i>(15) = 8.92, <i>p</i> &amp;lt; 0.001) and novices (<i>t</i>(14) = 6.48, <i>p</i> &amp;lt; 0.001) occurred in primary visual cortex (V1), and were localized to regions where both groups exhibited high representational convergence. Thus, the visual similarity analysis affirms that the mechanical category representations cannot be accounted for by a model of item-level visual similarity alone.</p> <p id="Par14">As a descriptive visualization to contextualize the data-driven informational networks, the average informational network DMs for each peak informational network are displayed at the stimulus item level in Fig. <a href="#Fig5">5</a>, produced through multidimensional scaling into a three-dimensional embedding space. Concentration probability ellipsoids are plotted for each mechanical category (R package "rgl"<sup><a href="#CR14">14</a></sup>). This visualization highlights the mechanical category structure that is discernable even in the dimensionality-reduced representations for the mechanical category informational gradient peaks. By contrast, mechanical category structure is not as clearly identifiable within the visual informational gradient peak representations (nor in the visual similarity model representation itself). </p><div class="fig" title="fig"> <div class="figure" title="figure"><a href="http://doi.org/"><span class="label" tagx="label" title="label">Fig. 5</span></a></div>   <div class="title" tagx="title" title="title">
Descriptive visualization showing that the mechanical categories are most discernable among the peak informational networks from the mechanical category analysis.</div> <p> In contrast, the visual similarity model and the peak informational networks from the visual similarity analysis are not optimized to dissociate the mechanical categories. Item-level representations are shown for mechanical and visual representational models (top row), peak mechanical category informational networks for each group (bottom left), and peak visual similarity informational networks for each group (bottom right). Plots were generated using a three-dimensional embedding from a nonmetric multidimensional scaling algorithm. Points are color-coded by stimulus mechanical category: cantilevers (green), trusses (blue), and vertical loads (orange). The stimuli in each category are also surrounded by an ellipsoid of concentration identifying the region in which points belonging to that category are likely to occur with 95% probability under the assumption that points are drawn from a trivariate normal distribution.</p>   </div> <p /> </div> <div class="evidencefortask-specificlearningbyfinalfmrirun" title="sec"> <div class="title" tagx="title" title="title">
Evidence for task-specific learning by final fMRI run</div> <p id="Par15">As discussed above, the primary results of this study derive from the first fMRI run, because group differences in neural activity at the first fMRI run can be attributed to interindividual differences in prior knowledge of physics and engineering concepts. However, to provide a thorough treatment of the data, we also analyzed the results of the fourth and final fMRI run using the same multivariate intersubject correlation and RSA methods applied to fMRI run 1. The results of this analysis show that the neural and behavioral changes that take place between run 1 and run 4 of the fMRI task were likely predominantly associated with task-specific learning (i.e., identifying the now-familiar stimulus as either the correct or incorrect exemplar), and were not clearly associated with changes in conceptual knowledge over the course of the experiment. Participants did still show distinct neural representations between groups at run 4 (Fig. <a href="#Fig6">6</a>), although there were fewer overall convergent representations for each group (15 unique informational networks for engineering students, 2 unique informational networks for novices, and 43 informational networks for each group in regions of overlapping representational convergence). Furthermore, regions of representational convergence overall were almost entirely localized to vOT, with just one posterior parietal region of convergence for each group in the right hemisphere and no convergent representations in dorsal premotor or primary motor areas. Finally, results from the RSA for run 4 revealed that those convergent representations that did emerge for each group were no longer as strongly reflective of mechanical category information (but remained as strongly reflective of visual feature information as at run 1; Supplementary Fig. <a href="#MOESM1">3</a>). </p><div class="fig" title="fig"> <div class="figure" title="figure"><a href="http://doi.org/"><span class="label" tagx="label" title="label">Fig. 6</span></a></div>   <div class="title" tagx="title" title="title">
Participants in each group show less convergent neural activity at the last fMRI run.</div> <p> Brain surface maps of the results from fMRI run 4 for the same analysis applied to fMRI run 1 in Fig. <a href="#Fig2">2</a>. Compared to run 1, participants at run 4 showed fewer regions where intersubject DM correlations at run 4 were above the chance threshold for engineering students only (green), novices only (blue), and both groups (red). Each group had some representational convergence in right posterior parietal cortex, but otherwise convergent representations were found only in vOT. Engineering students showed more anterior representational convergence in vOT than novices. Groups continued to overlap in representational convergence in occipital regions.</p>   </div> <p /> <p id="Par16">These results provide additional evidence that neural representations of Newtonian force concept knowledge are best identified at fMRI run 1. At the beginning of the experiment, the fact that there were group differences in neural representations at all <i>must</i> depend on the prior exposure to STEM education of the participants, because that is the only task-relevant dimension on which the participants varied upon beginning the experiment. By the end of the experiment, however, participants from both groups had improved on the behavioral task (although engineering students appear to encounter a ceiling effect; Fig. <a href="#Fig1">1</a>), despite not receiving any feedback about their task performance. This behavioral change is therefore not likely to derive from changes in conceptual understanding of physics and engineering, and indeed the observed changes in neural representational content are not consistent with an increase in the representation mechanical category information for either group (Fig. <a href="#Fig6">6</a>, Supplementary Fig. <a href="#MOESM1">3</a>). Thus, the results from fMRI run 4 reveal that our multivariate analytical approach is particularly sensitive to the representations that participants are using at a given point during the fMRI experiment, and fMRI run 1 is the only point in the experiment where participants’ representations can be reasonably inferred to derive from differences in STEM concept knowledge.</p> </div> </div> <div class="discussion" title="discussion"> <div class="title" tagx="title" title="title">
Discussion</div> <p id="Par17">The results of the present study demonstrate that classroom-based knowledge and experience inform the neural representations that students access when applying this knowledge in a naturalistic context. Our results show that engineering students exhibited convergent multivariate neural response patterns for real-world structures that strongly reflected information about the mechanical categories of those structures (Figs. <a href="#Fig2">2</a>–<a href="#Fig4">4</a>). This finding is additionally striking because no explicit mention of any structural categories was made at any point during the experiment. Therefore, viewing of the task stimuli in the context of mechanical force evaluation was sufficient to activate the conceptual category information that engineering students had previously learned.</p> <p id="Par18">Results from the informational network analysis and the FBD task further support the interpretation that these neural patterns reflect abstract concept knowledge. At the beginning of the experiment, while engineering students explicitly demonstrated knowledge about mechanical force (Fig. <a href="#Fig1">1b</a>; high FBD task accuracy), their brain activity implicitly reflected mechanical category information about the structures considered in the task (Fig. <a href="#Fig3">3</a>). In contrast, novices showed poor explicit understanding of mechanical force during the first fMRI run (Fig. <a href="#Fig1">1b</a>; low FBD task accuracy), and their brain activity during this run reflected no significant mechanical category representations (Fig. <a href="#Fig3">3</a>). Taken together, these results demonstrate that (1) abstract concept knowledge that engineering students acquire in school is activated in response to real-world stimuli, and (2) this learned categorical knowledge can be identified within their multivariate neural response patterns.</p> <p id="Par19">The group differences in neural representation of engineering concept knowledge become more pronounced when compared with the results of the visual similarity control (HMAX) analysis. Unlike mechanical category information, visual feature information was significantly represented in brain regions where both engineering students and novices showed convergence in their multivariate neural response patterns (Figs. <a href="#Fig2">2</a>–<a href="#Fig4">4</a>). These regions where both groups exhibited strong representations of item-level visual similarity were localized to posterior visual cortex, as expected. Notably, the anterior-to-posterior gradient of increasing visual similarity information in the ventral stream was opposite to the posterior-to-anterior gradient of increasing mechanical category information exhibited in the same region, leading us to conclude that visual and mechanical representations are supported by distinct brain regions, and that effective mechanical categorization of the stimulus items cannot be achieved using only the items’ visual feature similarities.</p> <p id="Par20">In terms of the localization of abstract mechanical force knowledge, it is interesting that we observe ventral stream results consistent with prior work<sup><a href="#CR3">3</a>–<a href="#CR5">5</a>,<a href="#CR15">15</a></sup>. Here, engineering students showed categorical concept knowledge represented in bilateral anterior vOT. We also observe a gradient of increasing mechanical category information in the dorsal stream, which is notable given prior research on the neural correlates of learned physics concepts (e.g., torque<sup><a href="#CR8">8</a></sup>, forces and causal motion<sup><a href="#CR10">10</a></sup>). In fact, these previous studies have localized explicit knowledge of linear and rotational force to many of the same specific regions we observe for the representation of mechanical force categories, including in particular M1, in which we observe representational convergence only for engineering students (Figs. <a href="#Fig2">2</a> and <a href="#Fig4">4</a>). An intriguing possibility—to be explored by further research—is that the involvement of M1 in representing abstract concept knowledge for engineering students might be a function of their prior hands-on experiences in their lab-based coursework. Such an interpretation would be consistent with the studies cited above, as well as with our finding that novices show only premotor representations and not primary motor representations.</p> <p id="Par21">An additional aspect of the representational localization results that is worth highlighting is the comparison between the multivariate representational analyses employed here and traditional univariate contrasts of neural activity levels between the two participant groups. A supplementary univariate analysis (detailed in <a href="#MOESM1">Supplementary Methods</a>) revealed a convergent finding that engineering students’ cognition during these tasks is supported by motor and premotor and parietal regions in a way that is not observed for novices (Supplementary Fig. <a href="#MOESM1">4</a>). However, that finding alone does not answer the question of where the item-level representations are housed and whether those representations differ between groups. Our multivariate analysis suggests that there is indeed a critical difference in item-level representations of conceptual information, but that those representational differences are most pronounced in the anterior ventral stream—a region that does not come out of the univariate general linear model (GLM). This difference between an analysis of global signal change and an analysis of representational geometry is precisely the advantage offered by MVPA techniques such as RSA.</p> <p id="Par22">One important feature distinguishing our current study from prior work is our use of naturalistic task stimuli in the form of real-world photographs, whereas these previous studies used fMRI tasks that were modeled after a specific lab task<sup><a href="#CR8">8</a></sup> or that resembled taking a physics test<sup><a href="#CR10">10</a></sup>. Furthermore, the dorsal stream regions we observe have been consistently implicated in the extensive body of neuroimaging and neuropsychology investigations of the neural system underlying tool use and object knowledge<sup><a href="#CR15">15</a>–<a href="#CR22">22</a></sup>.</p> <p id="Par23">The research presented in the current study is not without certain limitations. As with any cross-sectional study, it is possible that some of the differences we observe derive from individual differences that are not related to participants’ educational background. However, there are several reasons why we believe extraneous individual differences are unlikely to have influenced the primary findings in this study. First, participants in this study were all enrolled in a university with among the most highly selective admissions criteria in the United States. As a result, factors such as general cognitive ability and even specific proficiencies (e.g., as represented by SAT math scores) are of a highly restricted range for both groups of participants. Moreover, unlike other institutions where technical or engineering schools have separate admissions criteria from the liberal arts college, the institution where the research was conducted requires identical admissions criteria for students entering into any discipline, including engineering. This reduces the likelihood of selection bias in terms of admission to the population from which participants were drawn.</p> <p id="Par24">In the present study, we demonstrated that STEM knowledge that students learned in school influenced their neural representations of real-world objects. The multivariate neuroimaging approach used here effectively differentiated groups of students on the basis of their prior knowledge and experience in the domain of mechanical engineering. Moreover, beyond merely demonstrating similar neural patterns among engineering students, our analyses revealed expert-like engineering knowledge within these patterns. Using this and other similar approaches, future research will elucidate the neural underpinnings of knowledge in other STEM concept domains, as well as investigate changes in neural representations of concept knowledge as STEM learning takes place over time.</p> </div> <div class="methods" title="sec"> <div class="title" tagx="title" title="title">
Methods</div> <div class="participants" title="sec"> <div class="title" tagx="title" title="title">
Participants</div> <p id="Par25">Thirty-three students at Dartmouth College participated in this study. Two participants were dropped due to incomplete behavioral and scan data for a sample of <i>N</i> = 31 (<i>N</i><sub>female</sub> = 19; <i>M</i><sub>age</sub> = 20.65 years, SD = 1.70). Half of the participants (<i>N</i> = 15) had no background in engineering (referred to here as <i>novices</i>). Half (<i>N</i> = 16) were engineering students who, at the time of their participation, had taken or were nearly finished taking an engineering course intended for majors focusing on solid mechanics including a lab section (referred to here as <i>engineering students</i>). A lab-based course in advanced physics was a prerequisite for this engineering course. Of the engineering students, five had taken additional advanced structural engineering courses. Participants were recruited primarily through email listservs, provided informed consent prior to participation, and were compensated either with cash or curricular extra credit points. All protocols were approved by the Dartmouth Committee for the Protection of Human Subjects.</p> </div> <div class="stimulianddesign" title="sec"> <div class="title" tagx="title" title="title">
Stimuli and design</div> <p id="Par26">Stimuli for the behavioral and scanner tasks (i.e., the similarity probe and the FBD task, detailed in the section "Procedure") were 24 photographs of real-world, engineered structures. For each image, a component was selected to be the focal point of the FBD analysis, and that component was indicated with a red outline. For evaluation in the FBD task (described below), two more versions of each image were created, each labeled with arrows indicating the forces and moments acting on the component of interest. One arrow-labeled version was labeled correctly, and the other was labeled incorrectly, for use in the FBD task completed during scanning.</p> </div> <div class="freebodydiagram(fbd)task" title="sec"> <div class="title" tagx="title" title="title">
Free body diagram (FBD) task</div> <p id="Par27">Figure <a href="#Fig1">1a</a> illustrates the concept knowledge task regarding the interaction of Newtonian forces with real-world structures. Our goal was to design a task that elicited knowledge of Newtonian force, but for which mechanical category information about the structures was incidental. Therefore, we never mentioned the mechanical category names during the task, nor did we even refer to the existence of categories in the task instructions or at any time during the experiment.</p> <p id="Par28">In the scanner, participants completed the following analytical task concerning the equilibrium state of a segment of interest in each of 24 images of real-world structures: Participants saw each structure image first for 2 s without any additional markings, and then for 4 s with the component of interest outlined in red. This was followed by a jittered fixation period during which participants had been previously instructed to imagine the forces and moments acting on the component of interest to maintain static equilibrium in the system. Functional imaging data collected from this fixation/consideration period were those used in all neural analyses. Finally, the component-highlighted image reappeared for 4 s, this time with arrows labeling the forces and moments either correctly or incorrectly. Participants had to assess whether the labeling was correct or incorrect based on the model they had imagined during the fixation period, and indicate their evaluation via button press during the 4-s window. This was followed by one more jittered fixation period, to round off the duration of the trial at exactly 15.5 s. Finally, each trial was separated by 15.5 s of fixation to establish a baseline for the fMRI analysis.</p> <p id="Par29">Each participant completed four runs of the FBD task, and each run included all 24 stimulus images. On each run, 12 images (50%) were correctly labeled. The specific images that were correctly or incorrectly labeled varied pseudo-randomly from run to run such that by the end of the experiment, every participant had seen both the correct and incorrect versions of every stimulus twice.</p> </div> <div class="procedure" title="sec"> <div class="title" tagx="title" title="title">
Procedure</div> <p id="Par30">Participants completed two sessions within a period of at most 7 days: a behavioral session followed by functional magnetic resonance imaging (fMRI) scanner session. In the behavioral session, participants first completed a similarity ratings task where they rated the complete set of stimuli for nonspecific interitem similarity. They then completed two standardized tests of engineering and physics knowledge: the Statics Concept Inventory (SCI<sup><a href="#CR23">23</a></sup>) and the Force Concept Inventory (FCI<sup><a href="#CR24">24</a></sup>). Results of the similarity ratings task and the standardized evaluations are not discussed in the present study; see our prior work<sup><a href="#CR7">7</a></sup> for analysis and discussion of these results.</p> <p id="Par31">In the fMRI session, all participants were given an introduction to the concepts of Newtonian force, static equilibrium, and FBDs. After viewing the concept primer twice, they were shown the 24-item experimental stimulus set for familiarization and completed practice trials of the FBD task with a separate set of practice stimuli. Participants then completed the FBD task in the scanner over the course of four functional runs. Feedback ("correct" or "incorrect") was given for practice trials, but not for experimental trials. At the end of the scanner session, participants completed a post-scan version of the similarity ratings task (results not discussed here).</p> </div> <div class="imagingacquisitionparameters" title="sec"> <div class="title" tagx="title" title="title">
Imaging acquisition parameters</div> <p id="Par32">Brain images were acquired using a 3T Philips Achieva Intera scanner with a 32-channel head coil, using gradient-echo echo-planar imaging. For functional scans, an 80 × 80 reconstruction matrix was used with a 240 mm<sup>2</sup> FOV for whole-brain coverage over 42 transverse slices (Flip angle = 90°; TE = 35 ms; TR = 2500 ms; 3 mm<sup>3</sup> voxels; no gap). Slices were acquired in an interleaved order. Data were collected over four functional runs, each run consisting of 298 volumes. Data from of these functional runs—the first and the last—were used in the present analyses. For each participant, a single high-resolution T1-weighted anatomical scan was also collected (TE = 3.72 ms; TR = 8.176 ms; voxel resolution = 0.938 × 0.938 × 1.0 mm).</p> </div> <div class="functionalimagepreprocessing" title="sec"> <div class="title" tagx="title" title="title">
Functional image preprocessing</div> <p id="Par33">Preprocessing of fMRI data was carried out using the FSL FEAT software package<sup><a href="#CR25">25</a>,<a href="#CR26">26</a></sup>. First, for each participant, a high-resolution T1-weighted anatomical image was skull-stripped using the FSL brain-extraction tool<sup><a href="#CR27">27</a></sup>. Then, for each functional run, skull-stripping, motion correction, slice timing correction, prewhitening, and highpass temporal filtering (cutoff at 100 s) were applied to the EPI volumes. The functional images were then registered to the participant’s individual anatomical volume using the FSL linear registration tool<sup><a href="#CR28">28</a>–<a href="#CR30">30</a></sup>.</p> <p id="Par34">Next, in order to calculate beta-value estimates for each stimulus, an item-level univariate regression model was computed for each functional run using the GLM. To accomplish this, an explanatory variable was set up to model the brain activity associated with each individual stimulus. Activity was sampled from the initial 6 s of each trial, during which participants were shown an image of the stimulus and asked to imagine the Newtonian forces acting upon a given section of the structure (see Fig. <a href="#Fig1">1a</a>: <i>analyzed period</i>). This portion of the trial was separated from the remainder of the trial by a jittered fixation period, to allow for an unconfounded estimate of the BOLD signal. In order to model this brain activity separately from the rest of the trial, during which the participant responded via button press, a separate explanatory variable was set up to model brain activity associated with the response period (4 s per trial) combined across all trials. Brain activity associated with the response period was excluded from further analysis. The beta estimates associated with the individual stimuli (first 6 s of each trial) were then used in a multivariate analysis, described below.</p> <p id="Par35">In the final preprocessing step, we used the Freesurfer recon-all suite<sup><a href="#CR31">31</a></sup>; <a href="http://surfer.nmr.mgh.harvard.edu">http://surfer.nmr.mgh.harvard.edu</a>) to carry out cortical surface reconstruction for each subject-level T1-weighted image. These cortical surfaces were then transformed to Surface Mapping (SUMA) format<sup><a href="#CR32">32</a></sup>; <a href="http://afni.nimh.nih.gov/afni/suma/">http://afni.nimh.nih.gov/afni/suma/</a>). Each participant’s cortical surface map was then fitted to standard mesh grids defined by an icosahedron with 32 linear divisions, yielding 20,484 nodes for the whole-brain cortical surface. Using sulcal alignment of each participant’s cortical surface to the common MNI template provided by Freesurfer, this inflated surface mapping allowed for anatomical correspondence between surface nodes across participants.</p> </div> <div class="multivariateneuralpatternanalysis" title="sec"> <div class="title" tagx="title" title="title">
Multivariate neural pattern analysis</div> <p id="Par36">Our goal was to determine whether the two groups of participants displayed unique neural activity patterns while processing the task stimuli, and to characterize and localize those patterns if they emerged. We employed a series of multivariate neuroimaging analyses detailed below, using the fMRI data collected during the first run of the FBD task (the point of maximal task performance disparity between the two groups). The analyses proceeded in four steps as follows:</p> <p id="Par37">Step 1: Identify emergent neural representations for each subject. First, we constructed for each subject an item-level dissimilarity matrix (DM) at every one of the 20,484 searchlight locations in the brain. This was achieved in python<sup><a href="#CR33">33</a></sup> using the PyMVPA toolkit<sup><a href="#CR34">34</a></sup> by calculating, for each searchlight node, every pairwise correlation of item-level beta estimates at that node. The result of this analytical step was a set of subject-level surface maps, where each surface node contained a DM reflecting that node’s representation of the stimulus set (Fig. <a href="#Fig2">2</a>, left panel).</p> <p id="Par38">Step 2: Intersubject correlations by group. To identify whether the two groups showed group-level convergence of neural representations, we then calculated intersubject correlations between the subject-level DM surface maps. The intersubject correlation analysis proceeded as follows: </p><ol> <li> <p id="Par39">We computed the pairwise intersubject DM correlation values within each group at every surface node on the brain.</p> </li> <li> <p id="Par40">We applied a Fisher <i>z</i>-transformation (using the hyperbolic arctangent) to each intersubject correlation value.</p> </li> <li> <p id="Par41">We computed the average intersubject correlation (now a <i>z</i> value) for each surface node. This yielded a whole-brain correlation map of average <i>z</i> values for each group at the first fMRI run (Supplementary Fig. <a href="#MOESM1">2</a>).</p> </li> <li> <p id="Par42">In order to threshold the whole-brain maps for correlation values we could reasonably attribute to noise, we applied the following threshold: because a negative intersubject correlation value in this type of multivariate analysis is most likely due to noise, we used the negative extent of the observed intersubject correlation values as an estimate of the noise distribution of the data. For each whole-brain correlation map we computed, the negative extent of the data was approximately <i>z</i> = −0.02, so we applied a noise threshold of <i>z</i> &amp;gt; 0.02 to the correlation maps. This threshold was further validated using permuted null distributions<sup><a href="#CR35">35</a></sup> (details of this permutation analysis are reported in <a href="#MOESM1">Supplementary Methods</a>).</p> </li> <li> <p id="Par43">Finally, to produce the visualization shown in Fig. <a href="#Fig2">2</a>, we compared the two group-level average intersubject correlation maps by simply overlaying them on a common cortical surface projection. Regions where both groups exhibited average intersubject correlation values of <i>z</i> &amp;gt; 0.02 were labeled as "overlapping" regions, while regions where only one group exhibited average intersubject correlation values of <i>z</i> &amp;gt; 0.02 were labeled as "distinct" regions. For visualization in Fig. <a href="#Fig2">2</a>, spatial clustering was also applied to the final cortical surface map using the AFNI SurfClust function<sup><a href="#CR36">36</a></sup> to retain only clusters of at least five contiguous surface nodes.</p> </li> </ol> <p /> <p id="Par44">The above steps were applied separately to DMs from the first and last fMRI runs.</p> <p id="Par45">Step 3: Informational network analysis. In order to query the brain for mechanical category information to identify any neural representations that reflected STEM concept knowledge, we performed an informational network analysis<sup><a href="#CR7">7</a></sup>. The analysis proceeded as follows: First, for each group, average node-level DMs were extracted from the distinct and overlapping brain regions defined in the intersubject correlation analysis in Step 2 (Fig. <a href="#Fig2">2</a>). Next, the average node-level DMs for each group were subjected to Ward hierarchical clustering separately for the engineering-student-specific, novice-specific, and overlapping regions shown in Fig. <a href="#Fig2">2</a>. The number of hierarchical clusters was determined using a split-half cross-validation procedure used by Connolly et al.<sup><a href="#CR37">37</a></sup> testing cluster solutions from 2 to 100 clusters over 1000 repetitions<sup><a href="#CR37">37</a></sup>. The clusters identified for each group within each set of distinct and overlapping brain regions are designated as informational networks, because they are defined according to their shared informational content without reference to any external or a priori representational model.</p> <p id="Par46">Having defined the informational networks at the group level, an RSA was performed at the individual participant level on the representations within each informational network. Separately for each participant, an average DM was computed for every informational network. Next, the normalized correlations were calculated between each informational network DM and a model DM representing the mechanical structure category for each stimulus item. Mechanical category information for the stimulus set was determined in consultation with a field expert (author SGD), and was never explicitly identified or discussed during the experiment to any participant. Normalized correlations were computed by first normalizing each informational network DM as well as the mechanical category target DM, and then computing the dot product of each informational network DM and the mechanical category DM. Finally, within each group, participant-level correlation distributions (<i>z</i>-values) across the informational networks were subjected to a one-sample <i>t</i>-test against zero to identify the degree to which each informational network’s representation consistently correlated with the mechanical or visual DM across the members of a group.</p> <p id="Par47">The group-level results from the one-sample <i>t</i>-tests were projected onto the cortical surface, yielding a surface map for each group showing the gradient of mechanical category information represented across the informational networks for each group (Fig. <a href="#Fig3">3</a>). As a supporting visualization of the item-level representations in each informational network, the average informational network DMs for the peak regions of mechanical category information from each group were extracted, and item-level representations were rendered using 3-dimensional nonmetric multidimensional scaling (MDS; "smacof" R package<sup><a href="#CR38">38</a></sup>). Those item-level MDS representations are displayed in Fig. <a href="#Fig5">5</a>.</p> <p id="Par48">Step 4: Visual similarity control model. To validate our analysis of mechanical information from neural activity patterns, we performed a control analysis using a model of visual similarity between items in the stimulus set. For this control analysis, we computed a visual similarity model of the stimuli using a forward-encoding model of primary visual cortex (HMAX, C1 layer<sup><a href="#CR13">13</a></sup>). RSA was then applied over the same informational networks defined in Step 3, using the identical approach as for the mechanical category model but this time targeting the visual similarity model. The surface maps for each group showing the correlations between neural representations and the visual similarity model for each group are displayed in Fig. <a href="#Fig3">3</a>, and item-level representations in the peak visual similarity regions are displayed using multidimensional scaling alongside the mechanical category information peaks and the mechanical and visual model DMs in Fig. <a href="#Fig5">5</a>. (The mechanical and visual RSA analyses were additionally computed for the last fMRI run, and the results are displayed in Supplementary Fig. <a href="#MOESM1">3</a>.)</p> </div> </div> <div class="supplementary-material" title="supplementary-material"> <div class="title" tagx="title" title="title">
Supplementary information</div> <div class="sec" title="sec"> <p> </p><div class="local-data" title="local-data"> <div class="media" title="media"><a href="41539_2020_65_MOESM1_ESM.pdf">LINK</a> <p>Supplementary Information</p>  </div> </div> <div class="local-data" title="local-data"> <div class="media" title="media"><a href="41539_2020_65_MOESM2_ESM.pdf">LINK</a> <p>nr-reporting-summary-cetron_kraemer_11_25_2019</p>  </div> </div> <p /> </div> </div> </div> <div class="back" title="back"> <div class="fn-group" title="fn-group"> <div class="fn-type-" title=""> <p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p> </div> </div> <div class="supplementaryinformation" title="sec"> <div class="title" tagx="title" title="title">
Supplementary information</div> <p><b>Supplementary information</b> is available for this paper at 10.1038/s41539-020-0065-x.</p> </div> <div class="ack" title="ack"> <div class="title" tagx="title" title="title">
Acknowledgements</div> <p>D.J.M.K. was supported by a National Science Foundation award (DRL-1661088) and by the Rockefeller Center for Public Policy at Dartmouth College. We thank the members of the Cognitive Neuroscience of Learning Lab at Dartmouth for feedback on earlier versions of this manuscript.</p> </div> <div class="title" tagx="title" title="title">
Author contributions</div> <p>J.S.C. and D.J.M.K. designed the study, developed stimuli, performed analysis, and wrote the manuscript. A.C.C., S.G.D., V.V.M., and J.V.H. contributed to study design, interpreting results, and writing the manuscript. A.C.C. also helped with development and evaluation of analysis methods.</p> <div class="title" tagx="title" title="title">
Data availability</div> <p>The data sets generated during the current study are available from the corresponding author on reasonable request.</p> <div class="title" tagx="title" title="title">
Code availability</div> <p>The code generated during the current study is available from the corresponding author on reasonable request.</p> <div class="title" tagx="title" title="title">
Competing interests</div> <p id="Par49">The authors declare no competing interests.</p> <div class="references">
References</div> <div tag="ref-list"> <ul> <div class="title" tagx="title" title="title">
References</div> <li tag="ref"><a name="CR1" /><span class="label" tagx="label" title="label">1.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chase</span><span class="given-names" tagx="given-names" title="given-names">W</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Simon</span><span class="given-names" tagx="given-names" title="given-names">H</span></span></span><span class="mixed-article-title" title="mixed-article-title">Perception in chess</span><span class="source" tagx="source" title="source">Cogn. Psychol.</span><span class="year" tagx="year" title="year">1973</span><span class="volume" tagx="volume" title="volume">4</span><span class="fpage" tagx="fpage" title="fpage">55</span><span class="lpage" tagx="lpage" title="lpage">81</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/0010-0285(73)90004-2">10.1016/0010-0285(73)90004-2</a></span></span></li> <li tag="ref"><a name="CR2" /><span class="label" tagx="label" title="label">2.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chi</span><span class="given-names" tagx="given-names" title="given-names">MTH</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Feltovich</span><span class="given-names" tagx="given-names" title="given-names">PJ</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Glaser</span><span class="given-names" tagx="given-names" title="given-names">R</span></span></span><span class="mixed-article-title" title="mixed-article-title">Categorization and representation of physics problems by experts and novices</span><span class="source" tagx="source" title="source">Cogn. Sci.</span><span class="year" tagx="year" title="year">1981</span><span class="volume" tagx="volume" title="volume">5</span><span class="fpage" tagx="fpage" title="fpage">121</span><span class="lpage" tagx="lpage" title="lpage">152</span><span class="pub-id"><a href="https://dx.doi.org/10.1207/s15516709cog0502_2">10.1207/s15516709cog0502_2</a></span></span></li> <li tag="ref"><a name="CR3" /><span class="label" tagx="label" title="label">3.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Connolly</span><span class="given-names" tagx="given-names" title="given-names">AC</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">The representation of biological classes in the human brain</span><span class="source" tagx="source" title="source">J. Neurosci.</span><span class="year" tagx="year" title="year">2012</span><span class="volume" tagx="volume" title="volume">32</span><span class="fpage" tagx="fpage" title="fpage">2608</span><span class="lpage" tagx="lpage" title="lpage">2618</span><span class="pub-id"><a href="https://dx.doi.org/10.1523/JNEUROSCI.5547-11.2012">10.1523/JNEUROSCI.5547-11.2012</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/22357845">22357845</a></span></span></li> <li tag="ref"><a name="CR4" /><span class="label" tagx="label" title="label">4.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Haxby</span><span class="given-names" tagx="given-names" title="given-names">JV</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">Distributed and overlapping representations of faces and objects in ventral temporal cortex</span><span class="source" tagx="source" title="source">Science</span><span class="year" tagx="year" title="year">2001</span><span class="volume" tagx="volume" title="volume">293</span><span class="fpage" tagx="fpage" title="fpage">2425</span><span class="lpage" tagx="lpage" title="lpage">2430</span><span class="pub-id"><a href="https://dx.doi.org/10.1126/science.1063736">10.1126/science.1063736</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/11577229">11577229</a></span></span></li> <li tag="ref"><a name="CR5" /><span class="label" tagx="label" title="label">5.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Martin</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wiggs</span><span class="given-names" tagx="given-names" title="given-names">CL</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ungerleider</span><span class="given-names" tagx="given-names" title="given-names">LG</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Haxby</span><span class="given-names" tagx="given-names" title="given-names">JV</span></span></span><span class="mixed-article-title" title="mixed-article-title">Neural correlates of category-specific knowledge</span><span class="source" tagx="source" title="source">Nature</span><span class="year" tagx="year" title="year">1996</span><span class="volume" tagx="volume" title="volume">379</span><span class="fpage" tagx="fpage" title="fpage">649</span><span class="lpage" tagx="lpage" title="lpage">652</span><span class="pub-id"><a href="https://dx.doi.org/10.1038/379649a0">10.1038/379649a0</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/8628399">8628399</a></span></span></li> <li tag="ref"><a name="CR6" /><span class="label" tagx="label" title="label">6.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Weber</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Thompson-Schill</span><span class="given-names" tagx="given-names" title="given-names">SL</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Osherson</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Haxby</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Parsons</span><span class="given-names" tagx="given-names" title="given-names">L</span></span></span><span class="mixed-article-title" title="mixed-article-title">Predicting judged similarity of natural categories from their neural representations</span><span class="source" tagx="source" title="source">Neuropsychologia</span><span class="year" tagx="year" title="year">2009</span><span class="volume" tagx="volume" title="volume">47</span><span class="fpage" tagx="fpage" title="fpage">859</span><span class="lpage" tagx="lpage" title="lpage">868</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.neuropsychologia.2008.12.029">10.1016/j.neuropsychologia.2008.12.029</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/19162048">19162048</a></span></span></li> <li tag="ref"><a name="CR7" /><span class="label" tagx="label" title="label">7.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Cetron</span><span class="given-names" tagx="given-names" title="given-names">JS</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">Decoding individual differences in STEM learning from functional MRI data</span><span class="source" tagx="source" title="source">Nat. Commun.</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">10</span><span class="fpage" tagx="fpage" title="fpage">2027</span><span class="pub-id"><a href="https://dx.doi.org/10.1038/s41467-019-10053-y">10.1038/s41467-019-10053-y</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/31048694">31048694</a></span></span></li> <li tag="ref"><a name="CR8" /><span class="label" tagx="label" title="label">8.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kontra</span><span class="given-names" tagx="given-names" title="given-names">C</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Lyons</span><span class="given-names" tagx="given-names" title="given-names">DJ</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Fischer</span><span class="given-names" tagx="given-names" title="given-names">SM</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Beilock</span><span class="given-names" tagx="given-names" title="given-names">SL</span></span></span><span class="mixed-article-title" title="mixed-article-title">Physical experience enhances science learning</span><span class="source" tagx="source" title="source">Psychol. Sci.</span><span class="year" tagx="year" title="year">2015</span><span class="volume" tagx="volume" title="volume">26</span><span class="fpage" tagx="fpage" title="fpage">737</span><span class="lpage" tagx="lpage" title="lpage">749</span><span class="pub-id"><a href="https://dx.doi.org/10.1177/0956797615569355">10.1177/0956797615569355</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/25911125">25911125</a></span></span></li> <li tag="ref"><a name="CR9" /><span class="label" tagx="label" title="label">9.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mason</span><span class="given-names" tagx="given-names" title="given-names">RA</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Just</span><span class="given-names" tagx="given-names" title="given-names">MA</span></span></span><span class="mixed-article-title" title="mixed-article-title">Physics instruction induces changes in neural knowledge representation during successive stages of learning</span><span class="source" tagx="source" title="source">NeuroImage</span><span class="year" tagx="year" title="year">2015</span><span class="volume" tagx="volume" title="volume">111</span><span class="fpage" tagx="fpage" title="fpage">36</span><span class="lpage" tagx="lpage" title="lpage">48</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.neuroimage.2014.12.086">10.1016/j.neuroimage.2014.12.086</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/25665967">25665967</a></span></span></li> <li tag="ref"><a name="CR10" /><span class="label" tagx="label" title="label">10.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mason</span><span class="given-names" tagx="given-names" title="given-names">RA</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Just</span><span class="given-names" tagx="given-names" title="given-names">MA</span></span></span><span class="mixed-article-title" title="mixed-article-title">Neural representations of physics concepts</span><span class="source" tagx="source" title="source">Psychol. Sci.</span><span class="year" tagx="year" title="year">2016</span><span class="volume" tagx="volume" title="volume">27</span><span class="fpage" tagx="fpage" title="fpage">904</span><span class="lpage" tagx="lpage" title="lpage">913</span><span class="pub-id"><a href="https://dx.doi.org/10.1177/0956797616641941">10.1177/0956797616641941</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/27113732">27113732</a></span></span></li> <li tag="ref"><a name="CR11" /><span class="label" tagx="label" title="label">11.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Gauthier</span><span class="given-names" tagx="given-names" title="given-names">I</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Tarr</span><span class="given-names" tagx="given-names" title="given-names">MJ</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Anderson</span><span class="given-names" tagx="given-names" title="given-names">AW</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Skudlarski</span><span class="given-names" tagx="given-names" title="given-names">P</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Gore</span><span class="given-names" tagx="given-names" title="given-names">JC</span></span></span><span class="mixed-article-title" title="mixed-article-title">Activation of the middle fusiform ‘face area’ increases with expertise in recognizing novel objects</span><span class="source" tagx="source" title="source">Nat. Neurosci.</span><span class="year" tagx="year" title="year">1999</span><span class="volume" tagx="volume" title="volume">2</span><span class="fpage" tagx="fpage" title="fpage">568</span><span class="lpage" tagx="lpage" title="lpage">573</span><span class="pub-id"><a href="https://dx.doi.org/10.1038/9224">10.1038/9224</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/10448223">10448223</a></span></span></li> <li tag="ref"><a name="CR12" /><span class="label" tagx="label" title="label">12.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kriegeskorte</span><span class="given-names" tagx="given-names" title="given-names">N</span></span></span><span class="mixed-article-title" title="mixed-article-title">Representational similarity analysis—connecting the branches of systems neuroscience</span><span class="source" tagx="source" title="source">Front. Syst. Neurosci.</span><span class="year" tagx="year" title="year">2008</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/neuro.06.004.2008">10.3389/neuro.06.004.2008</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/19104670">19104670</a></span></span></li> <li tag="ref"><a name="CR13" /><span class="label" tagx="label" title="label">13.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Serre</span><span class="given-names" tagx="given-names" title="given-names">T</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Oliva</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Poggio</span><span class="given-names" tagx="given-names" title="given-names">T</span></span></span><span class="mixed-article-title" title="mixed-article-title">A feedforward architecture accounts for rapid categorization</span><span class="source" tagx="source" title="source">Proc. Natl. Acad. Sci. USA</span><span class="year" tagx="year" title="year">2007</span><span class="volume" tagx="volume" title="volume">104</span><span class="fpage" tagx="fpage" title="fpage">6424</span><span class="lpage" tagx="lpage" title="lpage">6429</span><span class="pub-id"><a href="https://dx.doi.org/10.1073/pnas.0700622104">10.1073/pnas.0700622104</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/17404214">17404214</a></span></span></li> <li tag="ref"><a name="CR14" /><span class="label" tagx="label" title="label">14.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Adler, D. &amp;amp; Murdoch, D. <i>rgl: 3D Visualization Using OpenGL</i>. <a href="https://cran.r-project.org/web/packages/rgl/rgl.pdf">https://cran.r-project.org/web/packages/rgl/rgl.pdf</a> (2019).</span></li> <li tag="ref"><a name="CR15" /><span class="label" tagx="label" title="label">15.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Martin</span><span class="given-names" tagx="given-names" title="given-names">A</span></span></span><span class="mixed-article-title" title="mixed-article-title">The representation of object concepts in the brain</span><span class="source" tagx="source" title="source">Annu. Rev. Psychol.</span><span class="year" tagx="year" title="year">2007</span><span class="volume" tagx="volume" title="volume">58</span><span class="fpage" tagx="fpage" title="fpage">25</span><span class="lpage" tagx="lpage" title="lpage">45</span><span class="pub-id"><a href="https://dx.doi.org/10.1146/annurev.psych.57.102904.190143">10.1146/annurev.psych.57.102904.190143</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/16968210">16968210</a></span></span></li> <li tag="ref"><a name="CR16" /><span class="label" tagx="label" title="label">16.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chao</span><span class="given-names" tagx="given-names" title="given-names">LL</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Martin</span><span class="given-names" tagx="given-names" title="given-names">A</span></span></span><span class="mixed-article-title" title="mixed-article-title">Representation of manipulable man-made objects in the dorsal stream</span><span class="source" tagx="source" title="source">NeuroImage</span><span class="year" tagx="year" title="year">2000</span><span class="volume" tagx="volume" title="volume">12</span><span class="fpage" tagx="fpage" title="fpage">478</span><span class="lpage" tagx="lpage" title="lpage">484</span><span class="pub-id"><a href="https://dx.doi.org/10.1006/nimg.2000.0635">10.1006/nimg.2000.0635</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/10988041">10988041</a></span></span></li> <li tag="ref"><a name="CR17" /><span class="label" tagx="label" title="label">17.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chao</span><span class="given-names" tagx="given-names" title="given-names">LL</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Weisberg</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Martin</span><span class="given-names" tagx="given-names" title="given-names">A</span></span></span><span class="mixed-article-title" title="mixed-article-title">Experience-dependent modulation of category-related cortical activity</span><span class="source" tagx="source" title="source">Cereb. Cortex</span><span class="year" tagx="year" title="year">2002</span><span class="volume" tagx="volume" title="volume">12</span><span class="fpage" tagx="fpage" title="fpage">545</span><span class="lpage" tagx="lpage" title="lpage">551</span><span class="pub-id"><a href="https://dx.doi.org/10.1093/cercor/12.5.545">10.1093/cercor/12.5.545</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/11950772">11950772</a></span></span></li> <li tag="ref"><a name="CR18" /><span class="label" tagx="label" title="label">18.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chrysikou</span><span class="given-names" tagx="given-names" title="given-names">EG</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Thompson-Schill</span><span class="given-names" tagx="given-names" title="given-names">SL</span></span></span><span class="mixed-article-title" title="mixed-article-title">Dissociable brain states linked to common and creative object use</span><span class="source" tagx="source" title="source">Hum. Brain Mapp.</span><span class="year" tagx="year" title="year">2011</span><span class="volume" tagx="volume" title="volume">32</span><span class="fpage" tagx="fpage" title="fpage">665</span><span class="lpage" tagx="lpage" title="lpage">675</span><span class="pub-id"><a href="https://dx.doi.org/10.1002/hbm.21056">10.1002/hbm.21056</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/20533561">20533561</a></span></span></li> <li tag="ref"><a name="CR19" /><span class="label" tagx="label" title="label">19.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Musz</span><span class="given-names" tagx="given-names" title="given-names">E</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Thompson-Schill</span><span class="given-names" tagx="given-names" title="given-names">SL</span></span></span><span class="mixed-article-title" title="mixed-article-title">Semantic variability predicts neural variability of object concepts</span><span class="source" tagx="source" title="source">Neuropsychologia</span><span class="year" tagx="year" title="year">2015</span><span class="volume" tagx="volume" title="volume">76</span><span class="fpage" tagx="fpage" title="fpage">41</span><span class="lpage" tagx="lpage" title="lpage">51</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.neuropsychologia.2014.11.029">10.1016/j.neuropsychologia.2014.11.029</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/25462197">25462197</a></span></span></li> <li tag="ref"><a name="CR20" /><span class="label" tagx="label" title="label">20.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Warrington</span><span class="given-names" tagx="given-names" title="given-names">EK</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mccarthy</span><span class="given-names" tagx="given-names" title="given-names">RA</span></span></span><span class="mixed-article-title" title="mixed-article-title">Categories of knowledge further fractionations and an attempted integration</span><span class="source" tagx="source" title="source">Brain</span><span class="year" tagx="year" title="year">1987</span><span class="volume" tagx="volume" title="volume">110</span><span class="fpage" tagx="fpage" title="fpage">1273</span><span class="lpage" tagx="lpage" title="lpage">1296</span><span class="pub-id"><a href="https://dx.doi.org/10.1093/brain/110.5.1273">10.1093/brain/110.5.1273</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/3676701">3676701</a></span></span></li> <li tag="ref"><a name="CR21" /><span class="label" tagx="label" title="label">21.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Warrington</span><span class="given-names" tagx="given-names" title="given-names">EK</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Shallice</span><span class="given-names" tagx="given-names" title="given-names">T</span></span></span><span class="mixed-article-title" title="mixed-article-title">Category specific semantic impairments</span><span class="source" tagx="source" title="source">Brain</span><span class="year" tagx="year" title="year">1984</span><span class="volume" tagx="volume" title="volume">107</span><span class="fpage" tagx="fpage" title="fpage">829</span><span class="lpage" tagx="lpage" title="lpage">853</span><span class="pub-id"><a href="https://dx.doi.org/10.1093/brain/107.3.829">10.1093/brain/107.3.829</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/6206910">6206910</a></span></span></li> <li tag="ref"><a name="CR22" /><span class="label" tagx="label" title="label">22.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Weisberg</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">van Turennout</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Martin</span><span class="given-names" tagx="given-names" title="given-names">A</span></span></span><span class="mixed-article-title" title="mixed-article-title">A neural system for learning about object function</span><span class="source" tagx="source" title="source">Cereb. Cortex</span><span class="year" tagx="year" title="year">2007</span><span class="volume" tagx="volume" title="volume">17</span><span class="fpage" tagx="fpage" title="fpage">513</span><span class="lpage" tagx="lpage" title="lpage">521</span><span class="pub-id"><a href="https://dx.doi.org/10.1093/cercor/bhj176">10.1093/cercor/bhj176</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/16581980">16581980</a></span></span></li> <li tag="ref"><a name="CR23" /><span class="label" tagx="label" title="label">23.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Steif</span><span class="given-names" tagx="given-names" title="given-names">PS</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Dantzler</span><span class="given-names" tagx="given-names" title="given-names">JA</span></span></span><span class="mixed-article-title" title="mixed-article-title">A statics concept inventory: development and psychometric analysis</span><span class="source" tagx="source" title="source">J. Eng. Educ.</span><span class="year" tagx="year" title="year">2005</span><span class="volume" tagx="volume" title="volume">94</span><span class="fpage" tagx="fpage" title="fpage">363</span><span class="lpage" tagx="lpage" title="lpage">371</span><span class="pub-id"><a href="https://dx.doi.org/10.1002/j.2168-9830.2005.tb00864.x">10.1002/j.2168-9830.2005.tb00864.x</a></span></span></li> <li tag="ref"><a name="CR24" /><span class="label" tagx="label" title="label">24.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Hestenes</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wells</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Swackhamer</span><span class="given-names" tagx="given-names" title="given-names">G</span></span></span><span class="mixed-article-title" title="mixed-article-title">Force concept inventory</span><span class="source" tagx="source" title="source">Phys. Teach.</span><span class="year" tagx="year" title="year">1992</span><span class="volume" tagx="volume" title="volume">30</span><span class="fpage" tagx="fpage" title="fpage">141</span><span class="lpage" tagx="lpage" title="lpage">158</span><span class="pub-id"><a href="https://dx.doi.org/10.1119/1.2343497">10.1119/1.2343497</a></span></span></li> <li tag="ref"><a name="CR25" /><span class="label" tagx="label" title="label">25.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Jenkinson</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Beckmann</span><span class="given-names" tagx="given-names" title="given-names">CF</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Behrens</span><span class="given-names" tagx="given-names" title="given-names">TEJ</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Woolrich</span><span class="given-names" tagx="given-names" title="given-names">MW</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Smith</span><span class="given-names" tagx="given-names" title="given-names">SM</span></span></span><span class="mixed-article-title" title="mixed-article-title">FSL</span><span class="source" tagx="source" title="source">NeuroImage</span><span class="year" tagx="year" title="year">2012</span><span class="volume" tagx="volume" title="volume">62</span><span class="fpage" tagx="fpage" title="fpage">782</span><span class="lpage" tagx="lpage" title="lpage">790</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.neuroimage.2011.09.015">10.1016/j.neuroimage.2011.09.015</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/21979382">21979382</a></span></span></li> <li tag="ref"><a name="CR26" /><span class="label" tagx="label" title="label">26.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Smith</span><span class="given-names" tagx="given-names" title="given-names">SM</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">Advances in functional and structural MR image analysis and implementation as FSL</span><span class="source" tagx="source" title="source">NeuroImage</span><span class="year" tagx="year" title="year">2004</span><span class="volume" tagx="volume" title="volume">23</span><span class="issue" tagx="issue" title="issue">Suppl 1</span><span class="fpage" tagx="fpage" title="fpage">S208</span><span class="lpage" tagx="lpage" title="lpage">S219</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.neuroimage.2004.07.051">10.1016/j.neuroimage.2004.07.051</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/15501092">15501092</a></span></span></li> <li tag="ref"><a name="CR27" /><span class="label" tagx="label" title="label">27.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Smith</span><span class="given-names" tagx="given-names" title="given-names">SM</span></span></span><span class="mixed-article-title" title="mixed-article-title">Fast robust automated brain extraction</span><span class="source" tagx="source" title="source">Hum. Brain Mapp.</span><span class="year" tagx="year" title="year">2002</span><span class="volume" tagx="volume" title="volume">17</span><span class="fpage" tagx="fpage" title="fpage">143</span><span class="lpage" tagx="lpage" title="lpage">155</span><span class="pub-id"><a href="https://dx.doi.org/10.1002/hbm.10062">10.1002/hbm.10062</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/12391568">12391568</a></span></span></li> <li tag="ref"><a name="CR28" /><span class="label" tagx="label" title="label">28.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Greve</span><span class="given-names" tagx="given-names" title="given-names">DN</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Fischl</span><span class="given-names" tagx="given-names" title="given-names">B</span></span></span><span class="mixed-article-title" title="mixed-article-title">Accurate and robust brain image alignment using boundary-based registration</span><span class="source" tagx="source" title="source">NeuroImage</span><span class="year" tagx="year" title="year">2009</span><span class="volume" tagx="volume" title="volume">48</span><span class="fpage" tagx="fpage" title="fpage">63</span><span class="lpage" tagx="lpage" title="lpage">72</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.neuroimage.2009.06.060">10.1016/j.neuroimage.2009.06.060</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/19573611">19573611</a></span></span></li> <li tag="ref"><a name="CR29" /><span class="label" tagx="label" title="label">29.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Jenkinson</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Smith</span><span class="given-names" tagx="given-names" title="given-names">S</span></span></span><span class="mixed-article-title" title="mixed-article-title">A global optimisation method for robust affine registration of brain images</span><span class="source" tagx="source" title="source">Med. Image Anal.</span><span class="year" tagx="year" title="year">2001</span><span class="volume" tagx="volume" title="volume">5</span><span class="fpage" tagx="fpage" title="fpage">143</span><span class="lpage" tagx="lpage" title="lpage">156</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/S1361-8415(01)00036-6">10.1016/S1361-8415(01)00036-6</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/11516708">11516708</a></span></span></li> <li tag="ref"><a name="CR30" /><span class="label" tagx="label" title="label">30.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Jenkinson</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Bannister</span><span class="given-names" tagx="given-names" title="given-names">P</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Brady</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Smith</span><span class="given-names" tagx="given-names" title="given-names">S</span></span></span><span class="mixed-article-title" title="mixed-article-title">Improved optimization for the robust and accurate linear registration and motion correction of brain images</span><span class="source" tagx="source" title="source">NeuroImage</span><span class="year" tagx="year" title="year">2002</span><span class="volume" tagx="volume" title="volume">17</span><span class="fpage" tagx="fpage" title="fpage">825</span><span class="lpage" tagx="lpage" title="lpage">841</span><span class="pub-id"><a href="https://dx.doi.org/10.1006/nimg.2002.1132">10.1006/nimg.2002.1132</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/12377157">12377157</a></span></span></li> <li tag="ref"><a name="CR31" /><span class="label" tagx="label" title="label">31.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Fischl</span><span class="given-names" tagx="given-names" title="given-names">B</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Dale</span><span class="given-names" tagx="given-names" title="given-names">AM</span></span></span><span class="mixed-article-title" title="mixed-article-title">Measuring the thickness of the human cerebral cortex from magnetic resonance images</span><span class="source" tagx="source" title="source">Proc. Natl. Acad. Sci. USA</span><span class="year" tagx="year" title="year">2000</span><span class="volume" tagx="volume" title="volume">97</span><span class="fpage" tagx="fpage" title="fpage">11050</span><span class="lpage" tagx="lpage" title="lpage">11055</span><span class="pub-id"><a href="https://dx.doi.org/10.1073/pnas.200033797">10.1073/pnas.200033797</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/10984517">10984517</a></span></span></li> <li tag="ref"><a name="CR32" /><span class="label" tagx="label" title="label">32.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Saad</span><span class="given-names" tagx="given-names" title="given-names">ZS</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Reynolds</span><span class="given-names" tagx="given-names" title="given-names">RC</span></span></span><span class="mixed-article-title" title="mixed-article-title">SUMA</span><span class="source" tagx="source" title="source">NeuroImage</span><span class="year" tagx="year" title="year">2012</span><span class="volume" tagx="volume" title="volume">62</span><span class="fpage" tagx="fpage" title="fpage">768</span><span class="lpage" tagx="lpage" title="lpage">773</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.neuroimage.2011.09.016">10.1016/j.neuroimage.2011.09.016</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/21945692">21945692</a></span></span></li> <li tag="ref"><a name="CR33" /><span class="label" tagx="label" title="label">33.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">van Rossum</span><span class="given-names" tagx="given-names" title="given-names">G</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">de Boer</span><span class="given-names" tagx="given-names" title="given-names">J</span></span></span><span class="mixed-article-title" title="mixed-article-title">Interactively testing remote servers using the Python programming language</span><span class="source" tagx="source" title="source">CWI Q.</span><span class="year" tagx="year" title="year">1991</span><span class="volume" tagx="volume" title="volume">4</span><span class="fpage" tagx="fpage" title="fpage">283</span><span class="lpage" tagx="lpage" title="lpage">304</span></span></li> <li tag="ref"><a name="CR34" /><span class="label" tagx="label" title="label">34.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Hanke</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">PyMVPA: a Python Toolbox for multivariate pattern analysis of fMRI data</span><span class="source" tagx="source" title="source">Neuroinformatics</span><span class="year" tagx="year" title="year">2009</span><span class="volume" tagx="volume" title="volume">7</span><span class="fpage" tagx="fpage" title="fpage">37</span><span class="lpage" tagx="lpage" title="lpage">53</span><span class="pub-id"><a href="https://dx.doi.org/10.1007/s12021-008-9041-y">10.1007/s12021-008-9041-y</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/19184561">19184561</a></span></span></li> <li tag="ref"><a name="CR35" /><span class="label" tagx="label" title="label">35.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Nili</span><span class="given-names" tagx="given-names" title="given-names">H</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">A toolbox for representational similarity analysis</span><span class="source" tagx="source" title="source">PLoS Comput. Biol.</span><span class="year" tagx="year" title="year">2014</span><span class="volume" tagx="volume" title="volume">10</span><span class="fpage" tagx="fpage" title="fpage">e1003553</span><span class="pub-id"><a href="https://dx.doi.org/10.1371/journal.pcbi.1003553">10.1371/journal.pcbi.1003553</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/24743308">24743308</a></span></span></li> <li tag="ref"><a name="CR36" /><span class="label" tagx="label" title="label">36.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Cox</span><span class="given-names" tagx="given-names" title="given-names">RW</span></span></span><span class="mixed-article-title" title="mixed-article-title">AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</span><span class="source" tagx="source" title="source">Comput. Biomed. Res.</span><span class="year" tagx="year" title="year">1996</span><span class="volume" tagx="volume" title="volume">29</span><span class="fpage" tagx="fpage" title="fpage">162</span><span class="lpage" tagx="lpage" title="lpage">173</span><span class="pub-id"><a href="https://dx.doi.org/10.1006/cbmr.1996.0014">10.1006/cbmr.1996.0014</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/8812068">8812068</a></span></span></li> <li tag="ref"><a name="CR37" /><span class="label" tagx="label" title="label">37.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Connolly</span><span class="given-names" tagx="given-names" title="given-names">AC</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">How the human brain represents perceived dangerousness or "predacity" of animals</span><span class="source" tagx="source" title="source">J. Neurosci.</span><span class="year" tagx="year" title="year">2016</span><span class="volume" tagx="volume" title="volume">36</span><span class="fpage" tagx="fpage" title="fpage">5373</span><span class="lpage" tagx="lpage" title="lpage">5384</span><span class="pub-id"><a href="https://dx.doi.org/10.1523/JNEUROSCI.3395-15.2016">10.1523/JNEUROSCI.3395-15.2016</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/27170133">27170133</a></span></span></li> <li tag="ref"><a name="CR38" /><span class="label" tagx="label" title="label">38.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Mair, P., Leeuw, J. D., Borg, I. &amp;amp; Groenen, P. J. F. <i>smacof: Multidimensional Scaling.</i><a href="https://cran.r-project.org/web/packages/smacof/smacof.pdf">https://cran.r-project.org/web/packages/smacof/smacof.pdf</a> (2018).</span></li> </ul> </div> </div>  </body></html>